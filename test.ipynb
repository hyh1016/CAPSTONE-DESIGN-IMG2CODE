{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from os.path import basename\n",
    "from pix2code.model.classes.Sampler import *\n",
    "from pix2code.model.classes.model.pix2code import *\n",
    "from pix2code.model.classes.model.pix2code_v1 import *\n",
    "from pix2code.model.classes.model.pix2code_v1_Bi_GRU import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\keras\\optimizer_v2\\rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# trained_weights_path = 'pix2code/bin'\n",
    "trained_weights_path = 'pix2code/bin/tag_extension/6_pix2code_all_tag_early'\n",
    "\n",
    "meta_dataset = np.load(\"{}/meta_dataset.npy\".format(trained_weights_path), allow_pickle=True)\n",
    "input_shape = meta_dataset[0]\n",
    "output_size = meta_dataset[1]\n",
    "\n",
    "model = pix2code_v1_Bi_GRU(input_shape, output_size, trained_weights_path)\n",
    "model.load('pix2code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 23\n",
      "Input shape: (256, 256, 3)\n",
      "Output size: 23\n"
     ]
    }
   ],
   "source": [
    "sampler = Sampler(trained_weights_path, input_shape, output_size, CONTEXT_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/2000 [00:21<2:26:01,  4.39s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:5128\u001b[0m, in \u001b[0;36mgather\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   5125\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   5126\u001b[0m   \u001b[39m# TODO(apassos) find a less bad way of detecting resource variables\u001b[39;00m\n\u001b[0;32m   5127\u001b[0m   \u001b[39m# without introducing a circular dependency.\u001b[39;00m\n\u001b[1;32m-> 5128\u001b[0m   \u001b[39mreturn\u001b[39;00m params\u001b[39m.\u001b[39;49msparse_read(indices, name\u001b[39m=\u001b[39mname)\n\u001b[0;32m   5129\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:442\u001b[0m, in \u001b[0;36mTensor.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    437\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m    438\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    439\u001b[0m \u001b[39m    If you are looking for numpy-related methods, please run the following:\u001b[39m\n\u001b[0;32m    440\u001b[0m \u001b[39m    from tensorflow.python.ops.numpy_ops import np_config\u001b[39m\n\u001b[0;32m    441\u001b[0m \u001b[39m    np_config.enable_numpy_behavior()\u001b[39m\u001b[39m\"\"\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n\u001b[1;32m--> 442\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'sparse_read'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\workspace\\im2code2\\capstone-design-img2code\\test.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/workspace/im2code2/capstone-design-img2code/test.ipynb#W3sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# print(file_name)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/workspace/im2code2/capstone-design-img2code/test.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m evaluation_img \u001b[39m=\u001b[39m Utils\u001b[39m.\u001b[39mget_preprocessed_img(input_path \u001b[39m+\u001b[39m input_file, IMAGE_SIZE)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/workspace/im2code2/capstone-design-img2code/test.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m result, _ \u001b[39m=\u001b[39m sampler\u001b[39m.\u001b[39;49mpredict_greedy(model, np\u001b[39m.\u001b[39;49marray([evaluation_img]))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/workspace/im2code2/capstone-design-img2code/test.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m result \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m<START>\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m<END>\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m{\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/workspace/im2code2/capstone-design-img2code/test.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00moutput_path\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00mfile_name\u001b[39m}\u001b[39;00m\u001b[39m.gui\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "File \u001b[1;32md:\\workspace\\im2code2\\capstone-design-img2code\\pix2code\\model\\classes\\Sampler.py:37\u001b[0m, in \u001b[0;36mSampler.predict_greedy\u001b[1;34m(self, model, input_img, require_sparse_label, sequence_length, verbose)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39mif\u001b[39;00m verbose:\n\u001b[0;32m     35\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mpredicting \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(i, sequence_length))\n\u001b[1;32m---> 37\u001b[0m probas \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(input_img, np\u001b[39m.\u001b[39;49marray([current_context]))\n\u001b[0;32m     38\u001b[0m prediction \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(probas)\n\u001b[0;32m     39\u001b[0m out_probas\u001b[39m.\u001b[39mappend(probas)\n",
      "File \u001b[1;32md:\\workspace\\im2code2\\capstone-design-img2code\\pix2code\\model\\classes\\model\\pix2code_v1_Bi_GRU.py:76\u001b[0m, in \u001b[0;36mpix2code_v1_Bi_GRU.predict\u001b[1;34m(self, image, partial_caption)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, image, partial_caption):\n\u001b[1;32m---> 76\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict([image, partial_caption], verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\keras\\engine\\training.py:1758\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1750\u001b[0m   \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m   1751\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1752\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mUsing Model.predict with \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1753\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mMultiWorkerDistributionStrategy or TPUStrategy and \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1754\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mAutoShardPolicy.FILE might lead to out-of-order result\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m   1755\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m. Consider setting it to AutoShardPolicy.DATA.\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1756\u001b[0m         stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m-> 1758\u001b[0m data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39;49mget_data_handler(\n\u001b[0;32m   1759\u001b[0m     x\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m   1760\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   1761\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps,\n\u001b[0;32m   1762\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m   1763\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m   1764\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1765\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1766\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1767\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1768\u001b[0m     steps_per_execution\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution)\n\u001b[0;32m   1770\u001b[0m \u001b[39m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   1771\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(callbacks, callbacks_module\u001b[39m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\keras\\engine\\data_adapter.py:1403\u001b[0m, in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(kwargs[\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39m_cluster_coordinator\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1402\u001b[0m   \u001b[39mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m-> 1403\u001b[0m \u001b[39mreturn\u001b[39;00m DataHandler(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\keras\\engine\\data_adapter.py:1153\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1150\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution \u001b[39m=\u001b[39m steps_per_execution\n\u001b[0;32m   1152\u001b[0m adapter_cls \u001b[39m=\u001b[39m select_data_adapter(x, y)\n\u001b[1;32m-> 1153\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter \u001b[39m=\u001b[39m adapter_cls(\n\u001b[0;32m   1154\u001b[0m     x,\n\u001b[0;32m   1155\u001b[0m     y,\n\u001b[0;32m   1156\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   1157\u001b[0m     steps\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[0;32m   1158\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs \u001b[39m-\u001b[39;49m initial_epoch,\n\u001b[0;32m   1159\u001b[0m     sample_weights\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1160\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m   1161\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1162\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1163\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1164\u001b[0m     distribution_strategy\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mdistribute\u001b[39m.\u001b[39;49mget_strategy(),\n\u001b[0;32m   1165\u001b[0m     model\u001b[39m=\u001b[39;49mmodel)\n\u001b[0;32m   1167\u001b[0m strategy \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mget_strategy()\n\u001b[0;32m   1169\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\keras\\engine\\data_adapter.py:327\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    323\u001b[0m   \u001b[39mreturn\u001b[39;00m flat_dataset\n\u001b[0;32m    325\u001b[0m indices_dataset \u001b[39m=\u001b[39m indices_dataset\u001b[39m.\u001b[39mflat_map(slice_batch_indices)\n\u001b[1;32m--> 327\u001b[0m dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mslice_inputs(indices_dataset, inputs)\n\u001b[0;32m    329\u001b[0m \u001b[39mif\u001b[39;00m shuffle \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    330\u001b[0m   \u001b[39mdef\u001b[39;00m \u001b[39mshuffle_batch\u001b[39m(\u001b[39m*\u001b[39mbatch):\n",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\keras\\engine\\data_adapter.py:359\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.slice_inputs\u001b[1;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgrab_batch\u001b[39m(i, data):\n\u001b[0;32m    357\u001b[0m   \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mmap_structure(\u001b[39mlambda\u001b[39;00m d: tf\u001b[39m.\u001b[39mgather(d, i, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), data)\n\u001b[1;32m--> 359\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mmap(\n\u001b[0;32m    360\u001b[0m     grab_batch, num_parallel_calls\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mAUTOTUNE)\n\u001b[0;32m    362\u001b[0m \u001b[39m# Default optimizations are disabled to avoid the overhead of (unnecessary)\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[39m# input pipeline graph serialization and deserialization\u001b[39;00m\n\u001b[0;32m    364\u001b[0m options \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mOptions()\n",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:2006\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[1;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[0;32m   2004\u001b[0m   \u001b[39mreturn\u001b[39;00m MapDataset(\u001b[39mself\u001b[39m, map_func, preserve_cardinality\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, name\u001b[39m=\u001b[39mname)\n\u001b[0;32m   2005\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 2006\u001b[0m   \u001b[39mreturn\u001b[39;00m ParallelMapDataset(\n\u001b[0;32m   2007\u001b[0m       \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   2008\u001b[0m       map_func,\n\u001b[0;32m   2009\u001b[0m       num_parallel_calls,\n\u001b[0;32m   2010\u001b[0m       deterministic,\n\u001b[0;32m   2011\u001b[0m       preserve_cardinality\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m   2012\u001b[0m       name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:5501\u001b[0m, in \u001b[0;36mParallelMapDataset.__init__\u001b[1;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[0;32m   5499\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_dataset \u001b[39m=\u001b[39m input_dataset\n\u001b[0;32m   5500\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_inter_op_parallelism \u001b[39m=\u001b[39m use_inter_op_parallelism\n\u001b[1;32m-> 5501\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func \u001b[39m=\u001b[39m StructuredFunctionWrapper(\n\u001b[0;32m   5502\u001b[0m     map_func,\n\u001b[0;32m   5503\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transformation_name(),\n\u001b[0;32m   5504\u001b[0m     dataset\u001b[39m=\u001b[39;49minput_dataset,\n\u001b[0;32m   5505\u001b[0m     use_legacy_function\u001b[39m=\u001b[39;49muse_legacy_function)\n\u001b[0;32m   5506\u001b[0m \u001b[39mif\u001b[39;00m deterministic \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   5507\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deterministic \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:4533\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m   4526\u001b[0m       warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   4527\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   4528\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   4529\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   4530\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   4531\u001b[0m     fn_factory \u001b[39m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[1;32m-> 4533\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function \u001b[39m=\u001b[39m fn_factory()\n\u001b[0;32m   4534\u001b[0m \u001b[39m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[0;32m   4535\u001b[0m add_to_graph \u001b[39m&\u001b[39m\u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3244\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3235\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_concrete_function\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   3236\u001b[0m   \u001b[39m\"\"\"Returns a `ConcreteFunction` specialized to inputs and execution context.\u001b[39;00m\n\u001b[0;32m   3237\u001b[0m \n\u001b[0;32m   3238\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3242\u001b[0m \u001b[39m       or `tf.Tensor` or `tf.TensorSpec`.\u001b[39;00m\n\u001b[0;32m   3243\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3244\u001b[0m   graph_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_concrete_function_garbage_collected(\n\u001b[0;32m   3245\u001b[0m       \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   3246\u001b[0m   graph_function\u001b[39m.\u001b[39m_garbage_collector\u001b[39m.\u001b[39mrelease()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   3247\u001b[0m   \u001b[39mreturn\u001b[39;00m graph_function\n",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3210\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3208\u001b[0m   args, kwargs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   3209\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m-> 3210\u001b[0m   graph_function, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n\u001b[0;32m   3211\u001b[0m   seen_names \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[0;32m   3212\u001b[0m   captured \u001b[39m=\u001b[39m object_identity\u001b[39m.\u001b[39mObjectIdentitySet(\n\u001b[0;32m   3213\u001b[0m       graph_function\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39minternal_captures)\n",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3557\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3553\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_define_function_with_shape_relaxation(\n\u001b[0;32m   3554\u001b[0m       args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[0;32m   3556\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_cache\u001b[39m.\u001b[39mmissed\u001b[39m.\u001b[39madd(call_context_key)\n\u001b[1;32m-> 3557\u001b[0m graph_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_graph_function(args, kwargs)\n\u001b[0;32m   3558\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_cache\u001b[39m.\u001b[39mprimary[cache_key] \u001b[39m=\u001b[39m graph_function\n\u001b[0;32m   3560\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function, filtered_flat_args\n",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3392\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3387\u001b[0m missing_arg_names \u001b[39m=\u001b[39m [\n\u001b[0;32m   3388\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (arg, i) \u001b[39mfor\u001b[39;00m i, arg \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(missing_arg_names)\n\u001b[0;32m   3389\u001b[0m ]\n\u001b[0;32m   3390\u001b[0m arg_names \u001b[39m=\u001b[39m base_arg_names \u001b[39m+\u001b[39m missing_arg_names\n\u001b[0;32m   3391\u001b[0m graph_function \u001b[39m=\u001b[39m ConcreteFunction(\n\u001b[1;32m-> 3392\u001b[0m     func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[0;32m   3393\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name,\n\u001b[0;32m   3394\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_function,\n\u001b[0;32m   3395\u001b[0m         args,\n\u001b[0;32m   3396\u001b[0m         kwargs,\n\u001b[0;32m   3397\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_signature,\n\u001b[0;32m   3398\u001b[0m         autograph\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph,\n\u001b[0;32m   3399\u001b[0m         autograph_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph_options,\n\u001b[0;32m   3400\u001b[0m         arg_names\u001b[39m=\u001b[39;49marg_names,\n\u001b[0;32m   3401\u001b[0m         override_flat_arg_shapes\u001b[39m=\u001b[39;49moverride_flat_arg_shapes,\n\u001b[0;32m   3402\u001b[0m         capture_by_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_capture_by_value),\n\u001b[0;32m   3403\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_attributes,\n\u001b[0;32m   3404\u001b[0m     function_spec\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_spec,\n\u001b[0;32m   3405\u001b[0m     \u001b[39m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[0;32m   3406\u001b[0m     \u001b[39m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[0;32m   3407\u001b[0m     \u001b[39m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[0;32m   3408\u001b[0m     \u001b[39m# ConcreteFunction.\u001b[39;00m\n\u001b[0;32m   3409\u001b[0m     shared_func_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m   3410\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\n",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1143\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[0;32m   1140\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1141\u001b[0m   _, original_func \u001b[39m=\u001b[39m tf_decorator\u001b[39m.\u001b[39munwrap(python_func)\n\u001b[1;32m-> 1143\u001b[0m func_outputs \u001b[39m=\u001b[39m python_func(\u001b[39m*\u001b[39;49mfunc_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfunc_kwargs)\n\u001b[0;32m   1145\u001b[0m \u001b[39m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[0;32m   1146\u001b[0m \u001b[39m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m   1147\u001b[0m func_outputs \u001b[39m=\u001b[39m nest\u001b[39m.\u001b[39mmap_structure(convert, func_outputs,\n\u001b[0;32m   1148\u001b[0m                                   expand_composites\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:4510\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.trace_tf_function.<locals>.wrapped_fn\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   4504\u001b[0m \u001b[39m@eager_function\u001b[39m\u001b[39m.\u001b[39mdefun_with_attributes(\n\u001b[0;32m   4505\u001b[0m     input_signature\u001b[39m=\u001b[39mstructure\u001b[39m.\u001b[39mget_flat_tensor_specs(\n\u001b[0;32m   4506\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_structure),\n\u001b[0;32m   4507\u001b[0m     autograph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   4508\u001b[0m     attributes\u001b[39m=\u001b[39mdefun_kwargs)\n\u001b[0;32m   4509\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_fn\u001b[39m(\u001b[39m*\u001b[39margs):  \u001b[39m# pylint: disable=missing-docstring\u001b[39;00m\n\u001b[1;32m-> 4510\u001b[0m   ret \u001b[39m=\u001b[39m wrapper_helper(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m   4511\u001b[0m   ret \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39mto_tensor_list(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_structure, ret)\n\u001b[0;32m   4512\u001b[0m   \u001b[39mreturn\u001b[39;00m [ops\u001b[39m.\u001b[39mconvert_to_tensor(t) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m ret]\n",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:4440\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.wrapper_helper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   4438\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _should_unpack(nested_args):\n\u001b[0;32m   4439\u001b[0m   nested_args \u001b[39m=\u001b[39m (nested_args,)\n\u001b[1;32m-> 4440\u001b[0m ret \u001b[39m=\u001b[39m autograph\u001b[39m.\u001b[39;49mtf_convert(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func, ag_ctx)(\u001b[39m*\u001b[39;49mnested_args)\n\u001b[0;32m   4441\u001b[0m \u001b[39mif\u001b[39;00m _should_pack(ret):\n\u001b[0;32m   4442\u001b[0m   ret \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(ret)\n",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:696\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    694\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    695\u001b[0m   \u001b[39mwith\u001b[39;00m conversion_ctx:\n\u001b[1;32m--> 696\u001b[0m     \u001b[39mreturn\u001b[39;00m converted_call(f, args, kwargs, options\u001b[39m=\u001b[39;49moptions)\n\u001b[0;32m    697\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    698\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:383\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    380\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[0;32m    382\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39muser_requested \u001b[39mand\u001b[39;00m conversion\u001b[39m.\u001b[39mis_allowlisted(f):\n\u001b[1;32m--> 383\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[0;32m    385\u001b[0m \u001b[39m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[39;00m\n\u001b[0;32m    386\u001b[0m \u001b[39m# call conversion from generated code while in nonrecursive mode. In that\u001b[39;00m\n\u001b[0;32m    387\u001b[0m \u001b[39m# case we evidently don't want to recurse, but we still have to convert\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[39m# things like builtins.\u001b[39;00m\n\u001b[0;32m    389\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39minternal_convert_user_code:\n",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:464\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    461\u001b[0m   \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39mcall(args, kwargs)\n\u001b[0;32m    463\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 464\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    465\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs)\n",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\keras\\engine\\data_adapter.py:357\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.slice_inputs.<locals>.grab_batch\u001b[1;34m(i, data)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgrab_batch\u001b[39m(i, data):\n\u001b[1;32m--> 357\u001b[0m   \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mnest\u001b[39m.\u001b[39;49mmap_structure(\u001b[39mlambda\u001b[39;49;00m d: tf\u001b[39m.\u001b[39;49mgather(d, i, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m), data)\n",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:869\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    865\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    866\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    868\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 869\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[0;32m    870\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:869\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    865\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    866\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    868\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 869\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[0;32m    870\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\keras\\engine\\data_adapter.py:357\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.slice_inputs.<locals>.grab_batch.<locals>.<lambda>\u001b[1;34m(d)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgrab_batch\u001b[39m(i, data):\n\u001b[1;32m--> 357\u001b[0m   \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mmap_structure(\u001b[39mlambda\u001b[39;00m d: tf\u001b[39m.\u001b[39;49mgather(d, i, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m), data)\n",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1096\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1094\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1095\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1096\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1097\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   1098\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:5141\u001b[0m, in \u001b[0;36mgather_v2\u001b[1;34m(params, indices, validate_indices, axis, batch_dims, name)\u001b[0m\n\u001b[0;32m   5133\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mgather\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[0;32m   5134\u001b[0m \u001b[39m@dispatch\u001b[39m\u001b[39m.\u001b[39madd_dispatch_support\n\u001b[0;32m   5135\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgather_v2\u001b[39m(params,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5139\u001b[0m               batch_dims\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[0;32m   5140\u001b[0m               name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m-> 5141\u001b[0m   \u001b[39mreturn\u001b[39;00m gather(\n\u001b[0;32m   5142\u001b[0m       params,\n\u001b[0;32m   5143\u001b[0m       indices,\n\u001b[0;32m   5144\u001b[0m       validate_indices\u001b[39m=\u001b[39;49mvalidate_indices,\n\u001b[0;32m   5145\u001b[0m       name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m   5146\u001b[0m       axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m   5147\u001b[0m       batch_dims\u001b[39m=\u001b[39;49mbatch_dims)\n",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1096\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1094\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1095\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1096\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1097\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   1098\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:552\u001b[0m, in \u001b[0;36mdeprecated_args.<locals>.deprecated_wrapper.<locals>.new_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    544\u001b[0m         _PRINTED_WARNING[(func, arg_name)] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    545\u001b[0m       logging\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m    546\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mFrom \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: calling \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m (from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m) with \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is deprecated and will \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    547\u001b[0m           \u001b[39m'\u001b[39m\u001b[39mbe removed \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mInstructions for updating:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    550\u001b[0m           \u001b[39m'\u001b[39m\u001b[39min a future version\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m date \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mafter \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m date),\n\u001b[0;32m    551\u001b[0m           instructions)\n\u001b[1;32m--> 552\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:5130\u001b[0m, in \u001b[0;36mgather\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   5128\u001b[0m   \u001b[39mreturn\u001b[39;00m params\u001b[39m.\u001b[39msparse_read(indices, name\u001b[39m=\u001b[39mname)\n\u001b[0;32m   5129\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[1;32m-> 5130\u001b[0m   \u001b[39mreturn\u001b[39;00m gen_array_ops\u001b[39m.\u001b[39;49mgather_v2(params, indices, axis, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:3951\u001b[0m, in \u001b[0;36mgather_v2\u001b[1;34m(params, indices, axis, batch_dims, name)\u001b[0m\n\u001b[0;32m   3949\u001b[0m   batch_dims \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m   3950\u001b[0m batch_dims \u001b[39m=\u001b[39m _execute\u001b[39m.\u001b[39mmake_int(batch_dims, \u001b[39m\"\u001b[39m\u001b[39mbatch_dims\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 3951\u001b[0m _, _, _op, _outputs \u001b[39m=\u001b[39m _op_def_library\u001b[39m.\u001b[39;49m_apply_op_helper(\n\u001b[0;32m   3952\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39mGatherV2\u001b[39;49m\u001b[39m\"\u001b[39;49m, params\u001b[39m=\u001b[39;49mparams, indices\u001b[39m=\u001b[39;49mindices, axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m   3953\u001b[0m                   batch_dims\u001b[39m=\u001b[39;49mbatch_dims, name\u001b[39m=\u001b[39;49mname)\n\u001b[0;32m   3954\u001b[0m _result \u001b[39m=\u001b[39m _outputs[:]\n\u001b[0;32m   3955\u001b[0m \u001b[39mif\u001b[39;00m _execute\u001b[39m.\u001b[39mmust_record_gradient():\n",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:512\u001b[0m, in \u001b[0;36m_apply_op_helper\u001b[1;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    506\u001b[0m       values \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mconvert_to_tensor(\n\u001b[0;32m    507\u001b[0m           values,\n\u001b[0;32m    508\u001b[0m           name\u001b[39m=\u001b[39minput_arg\u001b[39m.\u001b[39mname,\n\u001b[0;32m    509\u001b[0m           as_ref\u001b[39m=\u001b[39minput_arg\u001b[39m.\u001b[39mis_ref,\n\u001b[0;32m    510\u001b[0m           preferred_dtype\u001b[39m=\u001b[39mdefault_dtype)\n\u001b[0;32m    511\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 512\u001b[0m     values \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49mconvert_to_tensor(\n\u001b[0;32m    513\u001b[0m         values,\n\u001b[0;32m    514\u001b[0m         name\u001b[39m=\u001b[39;49minput_arg\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    515\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    516\u001b[0m         as_ref\u001b[39m=\u001b[39;49minput_arg\u001b[39m.\u001b[39;49mis_ref,\n\u001b[0;32m    517\u001b[0m         preferred_dtype\u001b[39m=\u001b[39;49mdefault_dtype)\n\u001b[0;32m    518\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    519\u001b[0m   \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py:163\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m   \u001b[39mwith\u001b[39;00m Trace(trace_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrace_kwargs):\n\u001b[0;32m    162\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 163\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1621\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1616\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mconvert_to_tensor did not convert to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1617\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mthe preferred dtype: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m vs \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1618\u001b[0m                       (ret\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype, preferred_dtype\u001b[39m.\u001b[39mbase_dtype))\n\u001b[0;32m   1620\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1621\u001b[0m   ret \u001b[39m=\u001b[39m conversion_func(value, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname, as_ref\u001b[39m=\u001b[39;49mas_ref)\n\u001b[0;32m   1623\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[0;32m   1624\u001b[0m   \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py:52\u001b[0m, in \u001b[0;36m_default_conversion_function\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_default_conversion_function\u001b[39m(value, dtype, name, as_ref):\n\u001b[0;32m     51\u001b[0m   \u001b[39mdel\u001b[39;00m as_ref  \u001b[39m# Unused.\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m   \u001b[39mreturn\u001b[39;00m constant_op\u001b[39m.\u001b[39;49mconstant(value, dtype, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:271\u001b[0m, in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[0;32m    175\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconstant\u001b[39m(value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConst\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    176\u001b[0m   \u001b[39m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \n\u001b[0;32m    178\u001b[0m \u001b[39m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[39m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 271\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    272\u001b[0m                         allow_broadcast\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:293\u001b[0m, in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    291\u001b[0m dtype_value \u001b[39m=\u001b[39m attr_value_pb2\u001b[39m.\u001b[39mAttrValue(\u001b[39mtype\u001b[39m\u001b[39m=\u001b[39mtensor_value\u001b[39m.\u001b[39mtensor\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m    292\u001b[0m attrs \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m: tensor_value, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m: dtype_value}\n\u001b[1;32m--> 293\u001b[0m const_tensor \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39;49m_create_op_internal(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    294\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mConst\u001b[39;49m\u001b[39m\"\u001b[39;49m, [], [dtype_value\u001b[39m.\u001b[39;49mtype], attrs\u001b[39m=\u001b[39;49mattrs, name\u001b[39m=\u001b[39;49mname)\u001b[39m.\u001b[39moutputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    296\u001b[0m \u001b[39mif\u001b[39;00m op_callbacks\u001b[39m.\u001b[39mshould_invoke_op_callbacks():\n\u001b[0;32m    297\u001b[0m   \u001b[39m# TODO(b/147670703): Once the special-op creation code paths\u001b[39;00m\n\u001b[0;32m    298\u001b[0m   \u001b[39m# are unified. Remove this `if` block.\u001b[39;00m\n\u001b[0;32m    299\u001b[0m   callback_outputs \u001b[39m=\u001b[39m op_callbacks\u001b[39m.\u001b[39minvoke_op_callbacks(\n\u001b[0;32m    300\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mConst\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mtuple\u001b[39m(), attrs, (const_tensor,), op_name\u001b[39m=\u001b[39mname, graph\u001b[39m=\u001b[39mg)\n",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:689\u001b[0m, in \u001b[0;36mFuncGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    687\u001b[0m   inp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcapture(inp)\n\u001b[0;32m    688\u001b[0m   captured_inputs\u001b[39m.\u001b[39mappend(inp)\n\u001b[1;32m--> 689\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(FuncGraph, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m_create_op_internal(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    690\u001b[0m     op_type, captured_inputs, dtypes, input_types, name, attrs, op_def,\n\u001b[0;32m    691\u001b[0m     compute_device)\n",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3697\u001b[0m, in \u001b[0;36mGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   3694\u001b[0m \u001b[39m# _create_op_helper mutates the new Operation. `_mutation_lock` ensures a\u001b[39;00m\n\u001b[0;32m   3695\u001b[0m \u001b[39m# Session.run call cannot occur between creating and mutating the op.\u001b[39;00m\n\u001b[0;32m   3696\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mutation_lock():\n\u001b[1;32m-> 3697\u001b[0m   ret \u001b[39m=\u001b[39m Operation(\n\u001b[0;32m   3698\u001b[0m       node_def,\n\u001b[0;32m   3699\u001b[0m       \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   3700\u001b[0m       inputs\u001b[39m=\u001b[39;49minputs,\n\u001b[0;32m   3701\u001b[0m       output_types\u001b[39m=\u001b[39;49mdtypes,\n\u001b[0;32m   3702\u001b[0m       control_inputs\u001b[39m=\u001b[39;49mcontrol_inputs,\n\u001b[0;32m   3703\u001b[0m       input_types\u001b[39m=\u001b[39;49minput_types,\n\u001b[0;32m   3704\u001b[0m       original_op\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_default_original_op,\n\u001b[0;32m   3705\u001b[0m       op_def\u001b[39m=\u001b[39;49mop_def)\n\u001b[0;32m   3706\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_op_helper(ret, compute_device\u001b[39m=\u001b[39mcompute_device)\n\u001b[0;32m   3707\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2097\u001b[0m, in \u001b[0;36mOperation.__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   2095\u001b[0m   \u001b[39mif\u001b[39;00m op_def \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2096\u001b[0m     op_def \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph\u001b[39m.\u001b[39m_get_op_def(node_def\u001b[39m.\u001b[39mop)\n\u001b[1;32m-> 2097\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_c_op \u001b[39m=\u001b[39m _create_c_op(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_graph, node_def, inputs,\n\u001b[0;32m   2098\u001b[0m                             control_input_ops, op_def)\n\u001b[0;32m   2099\u001b[0m   name \u001b[39m=\u001b[39m compat\u001b[39m.\u001b[39mas_str(node_def\u001b[39m.\u001b[39mname)\n\u001b[0;32m   2101\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_traceback \u001b[39m=\u001b[39m tf_stack\u001b[39m.\u001b[39mextract_stack_for_node(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_c_op)\n",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\dev\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1909\u001b[0m, in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def)\u001b[0m\n\u001b[0;32m   1907\u001b[0m inputs \u001b[39m=\u001b[39m _reconstruct_sequence_inputs(op_def, inputs, node_def\u001b[39m.\u001b[39mattr)\n\u001b[0;32m   1908\u001b[0m \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m-> 1909\u001b[0m op_desc \u001b[39m=\u001b[39m pywrap_tf_session\u001b[39m.\u001b[39;49mTF_NewOperation(graph\u001b[39m.\u001b[39;49m_c_graph,\n\u001b[0;32m   1910\u001b[0m                                             compat\u001b[39m.\u001b[39;49mas_str(node_def\u001b[39m.\u001b[39;49mop),\n\u001b[0;32m   1911\u001b[0m                                             compat\u001b[39m.\u001b[39;49mas_str(node_def\u001b[39m.\u001b[39;49mname))\n\u001b[0;32m   1912\u001b[0m \u001b[39mif\u001b[39;00m node_def\u001b[39m.\u001b[39mdevice:\n\u001b[0;32m   1913\u001b[0m   pywrap_tf_session\u001b[39m.\u001b[39mTF_SetDevice(op_desc, compat\u001b[39m.\u001b[39mas_str(node_def\u001b[39m.\u001b[39mdevice))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# dataGenerator/data/png/ 폴더 파일 리스트\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "input_path = 'dataGenerator/data/7/png/'\n",
    "output_path = 'dataGenerator/data/7/dsl_predict/'\n",
    "file_list = os.listdir(input_path)\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "for input_file in tqdm(file_list):\n",
    "    file_name = basename(input_file)[:basename(input_file).find(\".\")]\n",
    "    # print(file_name)\n",
    "    evaluation_img = Utils.get_preprocessed_img(input_path + input_file, IMAGE_SIZE)\n",
    "    result, _ = sampler.predict_greedy(model, np.array([evaluation_img]))\n",
    "    result = result.replace('<START>', '').replace('<END>', '').replace('{', ' {').replace(',', ', ')\n",
    "    with open(f'{output_path}{file_name}.gui', 'w') as f:\n",
    "        f.write(result)\n",
    "    # print(\"Result greedy: {}\".format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 22\n",
      "Input shape: (256, 256, 3)\n",
      "Output size: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [1:40:38<00:00,  3.02s/it]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "from os.path import basename\n",
    "from pix2code.model.classes.Sampler import *\n",
    "from pix2code.model.classes.model.pix2code import *\n",
    "from pix2code.model.classes.model.pix2code_v1 import *\n",
    "from pix2code.model.classes.model.pix2code_v1_Bi_GRU import *\n",
    "# trained_weights_path = 'pix2code/bin'\n",
    "trained_weights_path = 'pix2code/bin/tag_extension/3_button_color'\n",
    "\n",
    "meta_dataset = np.load(\"{}/meta_dataset.npy\".format(trained_weights_path), allow_pickle=True)\n",
    "input_shape = meta_dataset[0]\n",
    "output_size = meta_dataset[1]\n",
    "\n",
    "model = pix2code_v1_Bi_GRU(input_shape, output_size, trained_weights_path)\n",
    "model.load('pix2code')\n",
    "sampler = Sampler(trained_weights_path, input_shape, output_size, CONTEXT_LENGTH)\n",
    "# dataGenerator/data/png/ 폴더 파일 리스트\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "input_path = 'dataGenerator/data/3_color/png/'\n",
    "output_path = 'dataGenerator/data/3_color/dsl_predict/'\n",
    "file_list = os.listdir(input_path)\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "for input_file in tqdm(file_list):\n",
    "    file_name = basename(input_file)[:basename(input_file).find(\".\")]\n",
    "    # print(file_name)\n",
    "    evaluation_img = Utils.get_preprocessed_img(input_path + input_file, IMAGE_SIZE)\n",
    "    result, _ = sampler.predict_greedy(model, np.array([evaluation_img]))\n",
    "    result = result.replace('<START>', '').replace('<END>', '').replace('{', ' {').replace(',', ', ')\n",
    "    with open(f'{output_path}{file_name}.gui', 'w') as f:\n",
    "        f.write(result)\n",
    "    # print(\"Result greedy: {}\".format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 21\n",
      "Input shape: (256, 256, 3)\n",
      "Output size: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [1:52:25<00:00,  3.37s/it]  \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "from os.path import basename\n",
    "from pix2code.model.classes.Sampler import *\n",
    "from pix2code.model.classes.model.pix2code import *\n",
    "from pix2code.model.classes.model.pix2code_v1 import *\n",
    "from pix2code.model.classes.model.pix2code_v1_Bi_GRU import *\n",
    "# trained_weights_path = 'pix2code/bin'\n",
    "trained_weights_path = 'pix2code/bin/tag_extension/4_triple'\n",
    "\n",
    "meta_dataset = np.load(\"{}/meta_dataset.npy\".format(trained_weights_path), allow_pickle=True)\n",
    "input_shape = meta_dataset[0]\n",
    "output_size = meta_dataset[1]\n",
    "\n",
    "model = pix2code_v1_Bi_GRU(input_shape, output_size, trained_weights_path)\n",
    "model.load('pix2code')\n",
    "sampler = Sampler(trained_weights_path, input_shape, output_size, CONTEXT_LENGTH)\n",
    "# dataGenerator/data/png/ 폴더 파일 리스트\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "input_path = 'dataGenerator/data/4_triple/png/'\n",
    "output_path = 'dataGenerator/data/4_triple/dsl_predict/'\n",
    "file_list = os.listdir(input_path)\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "for input_file in tqdm(file_list):\n",
    "    file_name = basename(input_file)[:basename(input_file).find(\".\")]\n",
    "    # print(file_name)\n",
    "    evaluation_img = Utils.get_preprocessed_img(input_path + input_file, IMAGE_SIZE)\n",
    "    result, _ = sampler.predict_greedy(model, np.array([evaluation_img]))\n",
    "    result = result.replace('<START>', '').replace('<END>', '').replace('{', ' {').replace(',', ', ')\n",
    "    with open(f'{output_path}{file_name}.gui', 'w') as f:\n",
    "        f.write(result)\n",
    "    # print(\"Result greedy: {}\".format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 멀티 프로세싱 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from os.path import basename\n",
    "from pix2code.model.classes.Sampler import *\n",
    "from pix2code.model.classes.model.pix2code import *\n",
    "from pix2code.model.classes.model.pix2code_v1 import *\n",
    "from pix2code.model.classes.model.pix2code_v1_Bi_GRU import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_greedy_multi(input_img, inQueue:Queue, outQueue:Queue, id, voc:Vocabulary, input_shape, output_size, context_length, require_sparse_label=True, sequence_length=150, verbose=False):\n",
    "    current_context = [voc.vocabulary[PLACEHOLDER]] * (context_length - 1)\n",
    "    current_context.append(voc.vocabulary[START_TOKEN])\n",
    "    if require_sparse_label:\n",
    "        current_context = Utils.sparsify(current_context, output_size)\n",
    "\n",
    "    predictions = START_TOKEN\n",
    "    out_probas = []\n",
    "\n",
    "    for i in range(0, sequence_length):\n",
    "        if verbose:\n",
    "            print(\"predicting {}/{}...\".format(i, sequence_length))\n",
    "\n",
    "        # 멀티 프로세싱 처리\n",
    "        ##################\n",
    "        inQueue.put((input_img, np.array([current_context]), id))\n",
    "        while True:\n",
    "            if not outQueue.empty():\n",
    "                probases = outQueue.get()\n",
    "                break\n",
    "        probas = probases[id]\n",
    "        ##################\n",
    "\n",
    "        # probas = model.predict(input_img, np.array([current_context]))\n",
    "        prediction = np.argmax(probas)\n",
    "        out_probas.append(probas)\n",
    "\n",
    "        new_context = []\n",
    "        for j in range(1, context_length):\n",
    "            new_context.append(current_context[j])\n",
    "\n",
    "        if require_sparse_label:\n",
    "            sparse_label = np.zeros(output_size)\n",
    "            sparse_label[prediction] = 1\n",
    "            new_context.append(sparse_label)\n",
    "        else:\n",
    "            new_context.append(prediction)\n",
    "\n",
    "        current_context = new_context\n",
    "\n",
    "        predictions += voc.token_lookup[prediction]\n",
    "\n",
    "        if voc.token_lookup[prediction] == END_TOKEN:\n",
    "            break\n",
    "\n",
    "    return predictions, out_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Queue, Process, Pool\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# queue 입력 데이터\n",
    "# [input_img, current_context, qid]\n",
    "def doPredict(model:pix2code_v1_Bi_GRU, input_imgs, current_contexts, ids, outQueue:list):\n",
    "    result = model.predict_batch(input_imgs, current_contexts)\n",
    "    output = {}\n",
    "    for i in ids:\n",
    "        outQueue[i].put(result[i])\n",
    "\n",
    "def predictDsl(input_path, output_path, inQueue:Queue, outQueue:Queue, input_shape, output_size, qnum:int):\n",
    "    print(123, flush=True)\n",
    "    voc = Vocabulary()\n",
    "    voc.retrieve(trained_weights_path)\n",
    "    \n",
    "    file_list = os.listdir(input_path)\n",
    "    file_list = file_list[qnum::10]\n",
    "    for input_file in file_list:\n",
    "        file_name = basename(input_file)[:basename(input_file).find(\".\")]\n",
    "        evaluation_img = Utils.get_preprocessed_img(input_path + input_file, IMAGE_SIZE)\n",
    "        result, _ = predict_greedy_multi(evaluation_img, inQueue, outQueue, qnum, voc, input_shape, output_size, CONTEXT_LENGTH)\n",
    "        result = result.replace('<START>', '').replace('<END>', '').replace('{', ' {').replace(',', ', ')\n",
    "        with open(f'{output_path}{file_name}.gui', 'w') as f:\n",
    "            f.write(result)\n",
    "\n",
    "\n",
    "def worker(trained_weights_path:str, inQueue:mp.Queue, outQueue:mp.Queue):\n",
    "    meta_dataset = np.load(\"{}/meta_dataset.npy\".format(trained_weights_path), allow_pickle=True)\n",
    "    input_shape = meta_dataset[0]\n",
    "    output_size = meta_dataset[1]\n",
    "\n",
    "    model = pix2code_v1_Bi_GRU(input_shape, output_size, trained_weights_path)\n",
    "    model.load('pix2code')\n",
    "    \n",
    "    input_imgs = np.array()\n",
    "    current_contexts = np.array()\n",
    "    ids = []\n",
    "    cnt = 0\n",
    "    while True:\n",
    "        data = inQueue.get()\n",
    "        if data is None:\n",
    "            # 종료 조건\n",
    "            if cnt==0:\n",
    "                break\n",
    "            doPredict(model, input_imgs, current_contexts, ids, outQueue)\n",
    "            input_imgs = np.array()\n",
    "            current_contexts = np.array()\n",
    "            ids = []\n",
    "            cnt = 0\n",
    "            continue\n",
    "        input_imgs.append(data[0])\n",
    "        current_contexts.append(data[1])\n",
    "        ids.append(data[2])\n",
    "        cnt += 1\n",
    "        # 10개 찼을 때 처리\n",
    "        if cnt == 10:\n",
    "            doPredict(model, input_imgs, current_contexts, ids, outQueue)\n",
    "            input_imgs = np.array()\n",
    "            current_contexts = np.array()\n",
    "            ids = []\n",
    "            cnt = 0\n",
    "\n",
    "def main_process(inQueue:mp.Queue, outQueue:list, trained_weights_path:str, data_path:str):\n",
    "    meta_dataset = np.load(\"{}/meta_dataset.npy\".format(trained_weights_path), allow_pickle=True)\n",
    "    input_shape = meta_dataset[0]\n",
    "    output_size = meta_dataset[1]\n",
    "    input_path = f'{data_path}/png/'\n",
    "    output_path = f'{data_path}/dsl_predict/'\n",
    "\n",
    "    procs = []\n",
    "    for i in range(10):\n",
    "        # predictDsl(input_path, output_path, file_lists[i], inQueue, outQueue[i], input_shape, output_size, i)\n",
    "        # print(input_path, output_path, inQueue, outQueue[i], input_shape, output_size, i)\n",
    "        p = Process(target=predictDsl, args=(input_path, output_path, inQueue, outQueue[i], input_shape, output_size, i, ))\n",
    "        p.start()\n",
    "        # p.run()\n",
    "        procs.append(p)\n",
    "    \n",
    "    import time\n",
    "    # time.sleep(1)\n",
    "    # workerProc = Process(target=worker, args=(trained_weights_path, inQueue, outQueue))\n",
    "    # workerProc.start()\n",
    "\n",
    "    for p in procs:\n",
    "        p.join()\n",
    "    # workerProc.join()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    trained_weights_path='pix2code/bin/tag_extension/6_pix2code_all_tag_early'\n",
    "    data_path = 'dataGenerator/data/7'\n",
    "    inQueue = mp.Queue()\n",
    "    outQueue = [mp.Queue() for _ in range(10)]\n",
    "    main_process(inQueue, outQueue, trained_weights_path, data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "\n",
    "from os.path import basename\n",
    "from pix2code.model.classes.Sampler import *\n",
    "from pix2code.model.classes.model.pix2code import *\n",
    "from pix2code.model.classes.model.pix2code_v1 import *\n",
    "from pix2code.model.classes.model.pix2code_v1_Bi_GRU import *\n",
    "\n",
    "trained_weights_path = 'pix2code/bin/tag_extension/6_pix2code_all_tag_early'\n",
    "\n",
    "meta_dataset = np.load(\"{}/meta_dataset.npy\".format(trained_weights_path), allow_pickle=True)\n",
    "input_shape = meta_dataset[0]\n",
    "output_size = meta_dataset[1]\n",
    "\n",
    "model = pix2code_v1_Bi_GRU(input_shape, output_size, trained_weights_path)\n",
    "model.load('pix2code')\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "input_path = 'dataGenerator/data/7/png/'\n",
    "output_path = 'dataGenerator/data/7/dsl_predict/'\n",
    "\n",
    "\n",
    "file_list = os.listdir(input_path)\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "for input_file in tqdm(file_list):\n",
    "    file_name = basename(input_file)[:basename(input_file).find(\".\")]\n",
    "    # print(file_name)\n",
    "    evaluation_img = Utils.get_preprocessed_img(input_path + input_file, IMAGE_SIZE)\n",
    "    result, _ = sampler.predict_greedy(model, np.array([evaluation_img]))\n",
    "    result = result.replace('<START>', '').replace('<END>', '').replace('{', ' {').replace(',', ', ')\n",
    "    with open(f'{output_path}{file_name}.gui', 'w') as f:\n",
    "        f.write(result)\n",
    "    # print(\"Result greedy: {}\".format(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('capstone')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "416c1d3ac02f713a9a5502b63071185a8d76ad9cf71f6b904773d8aa1e5d8d6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
